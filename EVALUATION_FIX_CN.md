# 评测脚本中文字段支持说明

## 问题描述

原评测脚本 `inference/evaluate.py` 存在两个问题：

1. **字段名识别问题**：只支持英文字段名（如 `answer`、`answers`、`label` 等），无法识别中文数据集中的标准答案字段（如 `人工评测结果`），导致提示"缺少标准答案"。

2. **判定结果匹配问题**：对于事实核查任务，无法正确匹配中英文不同的表达方式（如 "成立" vs "T"、"不成立" vs "F"），导致评测准确率为 0。

## 解决方案

### 1. 中文字段支持

修改了 `get_ground_truth_answer` 函数，添加对中文字段的支持。

### 2. 判定结果标准化

新增 `normalize_verdict` 函数，支持中英文判定结果的标准化映射：

- **成立类**：T/True/成立/正确/支持/Yes → `T`
- **不成立类**：F/False/不成立/错误/不支持/No → `F`
- **不确定类**：uncertain/U/不确定/证据不足/无法判断 → `uncertain`

### 支持的中文字段名（按优先级）

1. **`original_row` 中的字段**:
   - `人工评测结果`
   - `标准答案`
   - `答案`
   - `label`

2. **根节点的字段**:
   - `人工评测结果`
   - `标准答案`
   - `答案`

3. **英文字段**（保持向后兼容）:
   - `answer`
   - `answers`
   - `answers_objects`
   - `label`

## 数据格式示例

### 中文数据集格式

```json
{
  "row_id": 0,
  "claim": "网页标题：pura80和mate70谁更优...",
  "original_row": {
    "query": 1.0,
    "claim": "网页内容...",
    "人工评测结果": "F",
    "10-02版本结果": "F"
  },
  "decomposed": [...],
  "retrieved_passages": {...},
  "final_answer": "生成的答案内容"
}
```

### 英文数据集格式（兼容）

```json
{
  "question": "What is the capital of France?",
  "answer": "Paris",
  "final_answer": "The capital of France is Paris."
}
```

## 使用方法

### 1. 运行评测

```bash
python inference/evaluate.py \
  --input_file your_results.jsonl \
  --dataset hotpotqa \
  --verbose
```

### 2. 参数说明

- `--input_file`: 包含生成结果和标准答案的 JSONL 文件
- `--dataset`: 数据集类型（hotpotqa、hover、exfever 等）
- `--verbose`: 显示详细对比信息
- `--output_file`: （可选）保存评测结果到文件

### 3. 输出示例

#### 标准评测输出
```
================================================================================
评测配置:
  输入文件: results.jsonl
  数据集: hotpotqa
================================================================================

正在加载生成结果...
加载了 5 条结果

开始评测...

================================================================================
评测结果:
================================================================================
总样本数: 5
有效评测样本数: 5

Exact Match (EM): 60.00%
F1 Score: 75.50%
================================================================================
```

#### 事实核查任务输出（含混淆矩阵）
```
================================================================================
评测结果:
================================================================================
总样本数: 10
有效评测样本数: 10

Exact Match (EM): 60.00%
F1 Score: 60.00%

================================================================================
混淆矩阵 (Confusion Matrix):
================================================================================
预测 \ 真实                  T          F  uncertain         总计
---------------------------------------------------------
T                        2          1          1          4
F                        1          2          0          3
uncertain                0          1          2          3
---------------------------------------------------------
总计                       3          4          3         10

================================================================================
各类别指标:
================================================================================
类别              Precision       Recall     F1-Score      Support
------------------------------------------------------------
T                  50.00%       66.67%       57.14%            3
F                  66.67%       50.00%       57.14%            4
uncertain          66.67%       66.67%       66.67%            3
================================================================================

总体准确率 (Accuracy): 60.00%
================================================================================
```

## 测试验证

### 测试 1：字段提取功能

```bash
python test_ground_truth.py
```

预期输出：
```
测试: original_row中的人工评测结果
✓ 通过 - 提取到: 'F'

测试: 根节点的人工评测结果
✓ 通过 - 提取到: 'T'

测试结果: 4 通过, 0 失败
```

### 测试 2：判定结果标准化

```bash
python test_verdict_normalization.py
```

预期输出：
```
✓ '成立' → 'T'
✓ '不成立' → 'F'
✓ '该主张不成立' → 'F'
✓ '证据不足' → 'uncertain'
...
测试结果: 25 通过, 0 失败

实际场景测试:
✓ 预测='成立' (T) vs 标准='T' (T) → 匹配=True
✓ 预测='不成立' (F) vs 标准='F' (F) → 匹配=True
```

### 测试 3：混淆矩阵功能

```bash
python test_confusion_matrix_standalone.py
```

预期输出：
```
样本详情:
  1. ✓ 预测=T, 真实=T
  2. ✗ 预测=F, 真实=T
  ...

混淆矩阵 (Confusion Matrix):
预测 \ 真实         T      F  uncertain   总计
--------------------------------------------------
T                   2      1          1      4
F                   1      2          0      3
uncertain           0      1          2      3
--------------------------------------------------
总计                3      4          3     10

各类别指标:
类别       Precision    Recall  F1-Score   Support
--------------------------------------------------
T            50.00%    66.67%    57.14%         3
F            66.67%    50.00%    57.14%         4
uncertain    66.67%    66.67%    66.67%         3

总体准确率 (Accuracy): 60.00%
```

## 混淆矩阵说明

对于事实核查任务，评测脚本会自动生成 3×3 混淆矩阵，展示预测标签与真实标签的对应关系。

### 混淆矩阵解读

混淆矩阵的行表示**预测标签**，列表示**真实标签**：

```
预测 \ 真实         T      F  uncertain
----------------------------------------
T                  TP     FP      FP
F                  FN     TP      FP
uncertain          FN     FN      TP
```

- **对角线**（TP）：预测正确的样本数
- **非对角线**：预测错误的样本数
  - 行方向（FP）：False Positive，将其他类别误判为当前类别
  - 列方向（FN）：False Negative，将当前类别误判为其他类别

### 各类别指标

对每个类别（T/F/uncertain），计算以下指标：

- **Precision（精确率）**：预测为该类别的样本中，真正属于该类别的比例
  - 公式：`TP / (TP + FP)`
  - 含义：预测的可靠性

- **Recall（召回率）**：真实为该类别的样本中，被正确预测的比例
  - 公式：`TP / (TP + FN)`
  - 含义：预测的覆盖度

- **F1-Score**：Precision 和 Recall 的调和平均数
  - 公式：`2 × (Precision × Recall) / (Precision + Recall)`
  - 含义：综合评价指标

- **Support**：该类别在真实标签中的样本总数

## 判定结果标准化详解

### 自动检测机制

评测脚本会自动检测是否是事实核查任务：

1. 如果数据集名称包含 "hover"、"exfever"、"fever"、"fact" 等关键词
2. 如果数据包含 `claim` 字段且答案为 T/F/uncertain 格式

检测到事实核查任务后，会：
- 使用判定结果标准化而非传统的文本标准化
- 自动生成混淆矩阵和各类别指标

### 标准化示例

| 预测答案 | 标准答案 | 标准化后 | 匹配结果 |
|---------|---------|---------|---------|
| "成立" | "T" | T vs T | ✓ 匹配 |
| "不成立" | "F" | F vs F | ✓ 匹配 |
| "证据不足" | "uncertain" | uncertain vs uncertain | ✓ 匹配 |
| "该主张成立" | "T" | T vs T | ✓ 匹配 |
| "该主张不成立" | "F" | F vs F | ✓ 匹配 |
| "成立" | "F" | T vs F | ✗ 不匹配 |

### 处理复杂回答

对于包含推理过程的回答，标准化函数会查找关键词：

```
回答: "<think>推理过程...</think> 综合以上分析，该主张成立。"
提取: "成立"
标准化: T
```

## 注意事项

1. **标准答案值**：
   - 对于事实核查任务，标准答案可能是 "T"（True）、"F"（False）或 "uncertain"
   - 生成的答案可以是中文（成立/不成立/不确定）或英文（True/False/Uncertain）
   - 评测时会自动进行标准化比较

2. **NaN 值处理**：
   - 如果字段值为 NaN（常见于 pandas DataFrame 转换的数据），会自动跳过

3. **优先级**：
   - 优先从 `original_row` 中提取，确保使用原始数据集的标准答案
   - 如果 `original_row` 中没有，再查找根节点字段

4. **关键词匹配顺序**：
   - 先匹配否定形式（"不成立"、"不支持"）避免误匹配
   - 再匹配肯定形式（"成立"、"支持"）

## 相关文件

- `inference/evaluate.py` - 评测主脚本（已修改）
- `test_ground_truth.py` - 测试脚本
- `inference/step2_generation.py` - 生成答案脚本（会保留 `original_row`）

## 修改日期

2025-11-11
